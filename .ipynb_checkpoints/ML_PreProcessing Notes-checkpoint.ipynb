{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8273308f",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "It involves below steps:\n",
    "- Getting the dataset\n",
    "- Importing libraries\n",
    "- Importing datasets\n",
    "- Finding Missing Data\n",
    "- Encoding Categorical Data\n",
    "- Splitting dataset into training and test set\n",
    "- Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca8100b",
   "metadata": {},
   "source": [
    "# 1. Getting the Dataset\n",
    "Is the process of getting your dataset ready\n",
    "\n",
    "# 2. Importing Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ce55b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f69f4db",
   "metadata": {},
   "source": [
    "# 3. Importing the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eeb76264",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\khali\\Downloads\\weather_forecast.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "559c8238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Outlook Temperature Humidity   Windy Play  Estimation\n",
      "0      Sunny         Hot     High    Weak   No        40.0\n",
      "1      Sunny         Hot     High  Strong   No        38.0\n",
      "2   Overcast         Hot     High    Weak  Yes        35.0\n",
      "3       Rain        Mild     High    Weak  Yes        32.0\n",
      "4       Rain        Cool   Normal    Weak  Yes        30.0\n",
      "5       Rain        Cool   Normal  Strong   No        28.0\n",
      "6   Overcast        Cool   Normal  Strong  Yes         NaN\n",
      "7      Sunny        Mild     High    Weak   No        33.0\n",
      "8      Sunny        Cool   Normal    Weak  Yes        35.0\n",
      "9       Rain        Mild   Normal    Weak  Yes        33.0\n",
      "10     Sunny        Mild   Normal  Strong  Yes        36.0\n",
      "11  Overcast        Mild     High  Strong  Yes         NaN\n",
      "12  Overcast         Hot   Normal    Weak  Yes        38.0\n",
      "13      Rain        Mild     High  Strong   No        34.0\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7758b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14 entries, 0 to 13\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Outlook      14 non-null     object \n",
      " 1   Temperature  14 non-null     object \n",
      " 2   Humidity     14 non-null     object \n",
      " 3   Windy        14 non-null     object \n",
      " 4   Play         14 non-null     object \n",
      " 5   Estimation   12 non-null     float64\n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 800.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db47f331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estimation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>34.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.446562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>34.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>36.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Estimation\n",
       "count   12.000000\n",
       "mean    34.333333\n",
       "std      3.446562\n",
       "min     28.000000\n",
       "25%     32.750000\n",
       "50%     34.500000\n",
       "75%     36.500000\n",
       "max     40.000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb8d87",
   "metadata": {},
   "source": [
    "## Extracting dependent and independent variables:\n",
    "In our dataset, there are three independent variables that are Country, Age, and Salary, and one is a dependent variable which is Purchased.\n",
    "\n",
    "### Extracting independent variable:\n",
    "\n",
    "To extract an independent variable, we will use iloc[ ] method of Pandas library. It is used to extract the required rows and columns from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5aadba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= data.iloc[:,:-1].values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbca1a",
   "metadata": {},
   "source": [
    "In the above code, the first colon(:) is used to take all the rows, and the second colon(:) is for all the columns. Here we have used :-1, because we don't want to take the last column as it contains the dependent variable. So by doing this, we will get the matrix of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8250c0",
   "metadata": {},
   "source": [
    "### Extracting dependent variable:\n",
    "\n",
    "To extract dependent variables, again, we will use Pandas .iloc[] method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "740ec382",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= data.iloc[:,3].values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "427b6575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sunny' 'Hot' 'High' 'Weak' 'No']\n",
      " ['Sunny' 'Hot' 'High' 'Strong' 'No']\n",
      " ['Overcast' 'Hot' 'High' 'Weak' 'Yes']\n",
      " ['Rain' 'Mild' 'High' 'Weak' 'Yes']\n",
      " ['Rain' 'Cool' 'Normal' 'Weak' 'Yes']\n",
      " ['Rain' 'Cool' 'Normal' 'Strong' 'No']\n",
      " ['Overcast' 'Cool' 'Normal' 'Strong' 'Yes']\n",
      " ['Sunny' 'Mild' 'High' 'Weak' 'No']\n",
      " ['Sunny' 'Cool' 'Normal' 'Weak' 'Yes']\n",
      " ['Rain' 'Mild' 'Normal' 'Weak' 'Yes']\n",
      " ['Sunny' 'Mild' 'Normal' 'Strong' 'Yes']\n",
      " ['Overcast' 'Mild' 'High' 'Strong' 'Yes']\n",
      " ['Overcast' 'Hot' 'Normal' 'Weak' 'Yes']\n",
      " ['Rain' 'Mild' 'High' 'Strong' 'No']]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f2e4a",
   "metadata": {},
   "source": [
    "# 4. Handling Missing data\n",
    "There are mainly two ways to handle missing data, which are:\n",
    "- Deleting the particular row\n",
    "- Calculating the mean\n",
    "\n",
    "To handle missing values, we will use $Scikit-learn library$ in our code, which contains various libraries for building machine learning models. Here we will use Imputer class of $sklearn.preprocessing$ library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "816ed810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate the independent variables (features) and dependent variable (target)\n",
    "x = df.iloc[:, :-1].values  # All columns except the last one\n",
    "y = df.iloc[:, -1].values   # The last column\n",
    "\n",
    "# Create the imputer object with the strategy to replace missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit the imputer to the 'Estimation' column and transform the data\n",
    "df['Estimation'] = imputer.fit_transform(df[['Estimation']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c234a8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Outlook Temperature Humidity   Windy Play  Estimation\n",
      "0      Sunny         Hot     High    Weak   No   40.000000\n",
      "1      Sunny         Hot     High  Strong   No   38.000000\n",
      "2   Overcast         Hot     High    Weak  Yes   35.000000\n",
      "3       Rain        Mild     High    Weak  Yes   32.000000\n",
      "4       Rain        Cool   Normal    Weak  Yes   30.000000\n",
      "5       Rain        Cool   Normal  Strong   No   28.000000\n",
      "6   Overcast        Cool   Normal  Strong  Yes   34.333333\n",
      "7      Sunny        Mild     High    Weak   No   33.000000\n",
      "8      Sunny        Cool   Normal    Weak  Yes   35.000000\n",
      "9       Rain        Mild   Normal    Weak  Yes   33.000000\n",
      "10     Sunny        Mild   Normal  Strong  Yes   36.000000\n",
      "11  Overcast        Mild     High  Strong  Yes   34.333333\n",
      "12  Overcast         Hot   Normal    Weak  Yes   38.000000\n",
      "13      Rain        Mild     High  Strong   No   34.000000\n"
     ]
    }
   ],
   "source": [
    "# Print the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b487e7",
   "metadata": {},
   "source": [
    "# 5. Encoding Categorical data\n",
    "Categorical data is data which has some categories such as, in our dataset; there are two categorical variable, Country, and Purchased.\n",
    "\n",
    "Categorical data is data which has some categories such as, in our dataset; there are two categorical variable, Country, and Purchased.\n",
    "\n",
    "For Country variable:\n",
    "\n",
    "Firstly, we will convert the country variables into categorical data. So to do this, we will use $LabelEncoder()$ class from preprocessing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "09ed2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Catgorical data  \n",
    "#for Country Variable  \n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "\n",
    "\n",
    "# Handling missing data for 'Estimation' column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['Estimation'] = imputer.fit_transform(df[['Estimation']])\n",
    "\n",
    "# Encoding categorical data for 'Temperature' and 'Windy' columns\n",
    "label_encoder_temperature = LabelEncoder()\n",
    "label_encoder_windy = LabelEncoder()\n",
    "\n",
    "df['Temperature'] = label_encoder_temperature.fit_transform(df['Temperature'])\n",
    "df['Windy'] = label_encoder_windy.fit_transform(df['Windy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d3e47f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Outlook  Temperature Humidity  Windy Play  Estimation\n",
      "0      Sunny            1     High      1   No   40.000000\n",
      "1      Sunny            1     High      0   No   38.000000\n",
      "2   Overcast            1     High      1  Yes   35.000000\n",
      "3       Rain            2     High      1  Yes   32.000000\n",
      "4       Rain            0   Normal      1  Yes   30.000000\n",
      "5       Rain            0   Normal      0   No   28.000000\n",
      "6   Overcast            0   Normal      0  Yes   34.333333\n",
      "7      Sunny            2     High      1   No   33.000000\n",
      "8      Sunny            0   Normal      1  Yes   35.000000\n",
      "9       Rain            2   Normal      1  Yes   33.000000\n",
      "10     Sunny            2   Normal      0  Yes   36.000000\n",
      "11  Overcast            2     High      0  Yes   34.333333\n",
      "12  Overcast            1   Normal      1  Yes   38.000000\n",
      "13      Rain            2     High      0   No   34.000000\n"
     ]
    }
   ],
   "source": [
    "# Print the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a494fa",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "In above code, we have imported LabelEncoder class of sklearn library. This class has successfully encoded the variables into digits.\n",
    "\n",
    "But in our case, there are three country variables, and as we can see in the above output, these variables are encoded into 0, 1, and 2. By these values, the machine learning model may assume that there is some correlation between these variables which will produce the wrong output. So to remove this issue, we will use dummy encoding.\n",
    "\n",
    "### Dummy Variables:\n",
    "\n",
    "Dummy variables are those variables which have values 0 or 1. The 1 value gives the presence of that variable in a particular column, and rest variables become 0. With dummy encoding, we will have a number of columns equal to the number of categories.\n",
    "\n",
    "In our dataset, we have 3 categories so it will produce three columns having 0 and 1 values. For Dummy Encoding, we will use $OneHotEncoder$ class of preprocessing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ddac4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#Encode Country Column\n",
    "labelencoder_x = LabelEncoder()\n",
    "x[:,0] = labelencoder_x.fit_transform(x[:,0])\n",
    "ct = ColumnTransformer([(\"Country\", OneHotEncoder(), [0])], remainder = 'passthrough')\n",
    "x = ct.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "990ce46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 1.0 0.0 0.0 1.0 'Hot' 'High' 'Weak' 'No']\n",
      " [1.0 0.0 1.0 0.0 0.0 1.0 'Hot' 'High' 'Strong' 'No']\n",
      " [0.0 1.0 0.0 1.0 0.0 0.0 'Hot' 'High' 'Weak' 'Yes']\n",
      " [1.0 0.0 1.0 0.0 1.0 0.0 'Mild' 'High' 'Weak' 'Yes']\n",
      " [1.0 0.0 1.0 0.0 1.0 0.0 'Cool' 'Normal' 'Weak' 'Yes']\n",
      " [1.0 0.0 1.0 0.0 1.0 0.0 'Cool' 'Normal' 'Strong' 'No']\n",
      " [0.0 1.0 0.0 1.0 0.0 0.0 'Cool' 'Normal' 'Strong' 'Yes']\n",
      " [1.0 0.0 1.0 0.0 0.0 1.0 'Mild' 'High' 'Weak' 'No']\n",
      " [1.0 0.0 1.0 0.0 0.0 1.0 'Cool' 'Normal' 'Weak' 'Yes']\n",
      " [1.0 0.0 1.0 0.0 1.0 0.0 'Mild' 'Normal' 'Weak' 'Yes']\n",
      " [1.0 0.0 1.0 0.0 0.0 1.0 'Mild' 'Normal' 'Strong' 'Yes']\n",
      " [0.0 1.0 0.0 1.0 0.0 0.0 'Mild' 'High' 'Strong' 'Yes']\n",
      " [0.0 1.0 0.0 1.0 0.0 0.0 'Hot' 'Normal' 'Weak' 'Yes']\n",
      " [1.0 0.0 1.0 0.0 1.0 0.0 'Mild' 'High' 'Strong' 'No']]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e217f",
   "metadata": {},
   "source": [
    "For Purchased Variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa919ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_y= LabelEncoder()  \n",
    "y= labelencoder_y.fit_transform(y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "de302293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 7 5 2 1 0 9 3 5 3 6 9 7 4]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19002087",
   "metadata": {},
   "source": [
    "For the second categorical variable, we will only use labelencoder object of LableEncoder class. Here we are not using OneHotEncoder class because the purchased variable has only two categories yes or no, and which are automatically encoded into 0 and 1."
   ]
  },
  {
   "attachments": {
    "data-preprocessing-machine-learning-5.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "36c013b6",
   "metadata": {},
   "source": [
    "# 6. Splitting the Dataset into the Training set and Test set\n",
    "\n",
    "In machine learning data preprocessing, we divide our dataset into a training set and test set. This is one of the crucial steps of data preprocessing as by doing this, we can enhance the performance of our machine learning model.\n",
    "\n",
    "Suppose, if we have given training to our machine learning model by a dataset and we test it by a completely different dataset. Then, it will create difficulties for our model to understand the correlations between the models.\n",
    "\n",
    "If we train our model very well and its training accuracy is also very high, but we provide a new dataset to it, then it will decrease the performance. So we always try to make a machine learning model which performs well with the training set and also with the test dataset. Here, we can define these datasets as:\n",
    "![data-preprocessing-machine-learning-5.png](attachment:data-preprocessing-machine-learning-5.png)\n",
    "\n",
    "- Training Set: A subset of dataset to train the machine learning model, and we already know the output.\n",
    "\n",
    "- Test set: A subset of dataset to test the machine learning model, and by using the test set, model predicts the output.\n",
    "\n",
    "For splitting the dataset, we will use the below lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9c043ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed3974c",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "In the above code, the first line is used for splitting arrays of the dataset into random train and test subsets.\n",
    "In the second line, we have used four variables for our output that are:\n",
    "\n",
    "- x_train: features for the training data\n",
    "- x_test: features for testing data\n",
    "- y_train: Dependent variables for training data\n",
    "- y_test: Independent variable for testing data\n",
    "\n",
    "In *train_test_split()* function, we have passed four parameters in which first two are for arrays of data, and test_size is for specifying the size of the test set. The *test_size* maybe .5, .3, or .2, which tells the dividing ratio of training and testing sets.\n",
    "The last parameter *random_state* is used to set a seed for a random generator so that you always get the same result, and the most used value for this is 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "43579215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 'Mild', 'High', 'Strong', 'Yes'],\n",
       "       [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 'Hot', 'High', 'Weak', 'Yes'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 'Mild', 'High', 'Strong', 'No'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 'Mild', 'Normal', 'Weak', 'Yes'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 'Hot', 'High', 'Strong', 'No'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 'Mild', 'High', 'Weak', 'No'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 'Mild', 'Normal', 'Strong', 'Yes'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 'Mild', 'High', 'Weak', 'Yes'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 'Hot', 'High', 'Weak', 'No'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 'Cool', 'Normal', 'Strong', 'No'],\n",
       "       [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 'Hot', 'Normal', 'Weak', 'Yes']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "23ce1d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 'Cool', 'Normal', 'Weak', 'Yes'],\n",
       "       [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 'Cool', 'Normal', 'Strong', 'Yes'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 'Cool', 'Normal', 'Weak', 'Yes']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c7ebb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 5, 4, 3, 7, 3, 6, 2, 8, 0, 7], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "db908bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 9, 1], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81716879",
   "metadata": {},
   "source": [
    "# 7. Feature Scaling\n",
    "Feature scaling is the final step of data preprocessing in machine learning. It is a technique to standardize the independent variables of the dataset in a specific range. In feature scaling, we put our variables in the same range and in the same scale so that no any variable dominate the other variable.\n",
    "\n",
    "For feature scaling, we will import StandardScaler class of sklearn.preprocessing library as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f49f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd580d",
   "metadata": {},
   "source": [
    "Now, we will create the object of $StandardScaler$ class for independent variables or features. And then we will fit and transform the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "402b1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (excluding non-numeric columns)\n",
    "numeric_features = df[['Estimation']]  # Assuming 'Estimation' is the only numeric column, adjust accordingly\n",
    "non_numeric_features = df.drop(columns=['Outlook', 'Temperature', 'Humidity', 'Windy', 'Play'])\n",
    "\n",
    "# Apply StandardScaler only to numeric features\n",
    "scaler = StandardScaler()\n",
    "numeric_features_scaled = scaler.fit_transform(numeric_features)\n",
    "\n",
    "# Concatenate scaled numeric features with non-numeric features\n",
    "x_scaled = np.concatenate((numeric_features_scaled, non_numeric_features.values), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "de15e444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.85485207 40.        ]\n",
      " [ 1.2001984  38.        ]\n",
      " [ 0.21821789 35.        ]\n",
      " [-0.76376262 32.        ]\n",
      " [-1.41841629 30.        ]\n",
      " [-2.07306996 28.        ]\n",
      " [ 0.         34.33333333]\n",
      " [-0.43643578 33.        ]\n",
      " [ 0.21821789 35.        ]\n",
      " [-0.43643578 33.        ]\n",
      " [ 0.54554473 36.        ]\n",
      " [ 0.         34.33333333]\n",
      " [ 1.2001984  38.        ]\n",
      " [-0.10910895 34.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print the scaled features\n",
    "print(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e56d34",
   "metadata": {},
   "source": [
    "For test dataset, we will directly apply $transform()$ function instead of *fit_transform()* because it is already done in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f10c5b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.910894511799619 1.0 0 1 0 0]\n",
      " [-11.238221347153608 0.0 1 0 1 0]\n",
      " [-11.238221347153608 1.0 0 1 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khali\\anaconda3\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Assuming x_test is defined and contains the test data\n",
    "\n",
    "# Extract the numeric feature 'Estimation' from the test data\n",
    "numeric_feature_test = x_test[:, 5].reshape(-1, 1)\n",
    "\n",
    "# Extract the non-numeric features (categorical) from the test data\n",
    "non_numeric_features_test = x_test[:, :5]  # Assuming the first 5 columns are non-numeric\n",
    "\n",
    "# Apply the transformation to the numeric feature using the previously fitted StandardScaler\n",
    "numeric_feature_test_scaled = scaler.transform(numeric_feature_test)\n",
    "\n",
    "# Combine the scaled numeric feature with the non-numeric features\n",
    "x_test_scaled = np.column_stack((numeric_feature_test_scaled, non_numeric_features_test))\n",
    "\n",
    "# Print the scaled test data\n",
    "print(x_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "25fd3194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 'Mild', 'High', 'Strong', 'Yes'],\n",
       "       [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 'Hot', 'High', 'Weak', 'Yes'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 'Mild', 'High', 'Strong', 'No'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 'Mild', 'Normal', 'Weak', 'Yes'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 'Hot', 'High', 'Strong', 'No'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 'Mild', 'High', 'Weak', 'No'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 'Mild', 'Normal', 'Strong', 'Yes'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 'Mild', 'High', 'Weak', 'Yes'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 'Hot', 'High', 'Weak', 'No'],\n",
       "       [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 'Cool', 'Normal', 'Strong', 'No'],\n",
       "       [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 'Hot', 'Normal', 'Weak', 'Yes']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "712640f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n",
       "       [0.0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1.0, 0, 1, 0, 1, 0, 0, 0, 1, 0]], dtype=object)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
